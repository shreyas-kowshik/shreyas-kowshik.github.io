---
title: "End of phase one"
subtitle: "Analysing what has been done and where we are..."
layout: post
tags: [machine-learning,generative-models,reinforcement-learning]
---

The last four weeks of the coding period were focused mostly on debugging the already implemented models and going forward with a new one. As I talked in the last post, `CycleGAN` and `pix2pix` were completed in terms of code within the community bonding period. So the next obvious step for me was to train the models. After spending the first couple of days debugging some small errors and getting the models up and running on the server, a bottleneck in terms of performance was observed for the entire network. 

The model performs a bit tad slow when compared with `pytorch` and when this would be accounted on a large scale, it could mean a shift of a few days in terms of training the entire network. So consequently I went forward debugging it. After talking to my mentors, they suggested me to integrate the [following patch](https://github.com/shreyas-kowshik/PPO.jl) to see if helped.

As of now, the patch for some reason performs worse than the current implementation. I'm still in the process of debugging it. In parallel, I was also trying to train `pix2pix` on small subset of the data to see if everything was working fine. After spending some time on it, I switched the discriminator from a pixel level calssifier to a patch classifier and this tended to give more visually descriptive results. I also reduced the generator network size to account for the training time issue and have currently put a batch on the server for training `pix2pix`. As for the front on `CycleGAN`, once `pix2pix` is known to be working well, I shall put up a batch for training on the server for that as well.

`Proximal Policy Optimization` has been completed. The model has been trained on `CartPole-v0` and `Pendulum-v0`, the former consisting of discrete action spaces while the latter continuous. 

The code and models are available [here](https://github.com/shreyas-kowshik/PPO.jl)
